{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 1\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "#Sample data\n",
    "x = np.array([22,44,21,56,77])\n",
    "y = np.array([56,88,90,33,32])\n",
    "#Define the model function(in this case, a linear function y=mx+c)\n",
    "def linear_model(params, x):\n",
    "    m,c = params\n",
    "    return m * x + c\n",
    "#Define the residuals function(difference between actual and predicted values)\n",
    "def residuals(params, y, x):\n",
    "    return y - linear_model(params, x)\n",
    "# Initial guess for the parameters (slope and intercept)\n",
    "initial_params = [1, 1]\n",
    "#Initial guess for the squares optimization\n",
    "result = leastsq(residuals, initial_params, args=(y,x))\n",
    "#Extract the optimized parameters\n",
    "m_opt, c_opt = result[0]\n",
    "#Print the results\n",
    "print(f\"Optimized slope(m): {m_opt}\")\n",
    "print(f\"Optimized intercept(c): {c_opt}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Partical 2\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "#Simulated data for demonstration\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.random.normal(n)\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "beta_true = [2.5, 1.7]\n",
    "y = beta_true[0] + beta_true[1] * x + epsilon\n",
    "#Define the moment condition function\n",
    "def moment_conditions(params, x, y):\n",
    "    beta0, beta1 = params\n",
    "    return y - beta0 - beta1 * x\n",
    "# Define the GMM objective function\n",
    "def gmm_objective(params, x, y):\n",
    "    moments = moment_conditions(params, x, y)\n",
    "    return moments.T @ moments\n",
    "# Initial guess for the parameters\n",
    "initial_guess = [0.5, 0.5]\n",
    "# Estimate parameters using GMM\n",
    "result = minimize(gmm_objective, initial_guess, args=(x, y))\n",
    "# Extract estimated parameters\n",
    "estimated_beta0, estimated_beta1 = result.x\n",
    "print(\"Estimated beta0:\", estimated_beta0)\n",
    "print(\"Estimated beta1:\", estimated_beta1)\n",
    "\n",
    "\n",
    "\n",
    "#Partical3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# Generate example of data\n",
    "np.random.seed(0)\n",
    "nobs = 10\n",
    "data = np.random.randn(nobs, 2)  #2 Variables\n",
    "#Convert data to DataFrame\n",
    "columns = ['variable1' , 'variable2']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "#Create lagged dataset\n",
    "lags = 2\n",
    "data_lagged = df.diff().dropna()\n",
    "for lag in range(1, lags + 1):\n",
    "    for col in columns:\n",
    "        df[f'{col}_lag{lag}']=data_lagged[col].shift(lag)\n",
    "#Drop rows with missing values\n",
    "df = df.dropna()\n",
    "#Split data into training and testing sets\n",
    "train_size = int(0.8 * len(df))\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]\n",
    "#Fit Ridge VAR model\n",
    "X_train = train_data.drop(columns=columns)\n",
    "y_train = train_data[columns]\n",
    "ridge_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0])  #Adjust aplhas as needed\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "#Forecast using the model\n",
    "X_test = test_data.drop(columns=columns)\n",
    "forecast = ridge_model.predict(X_test)\n",
    "#Convert forecast results to DataFrame\n",
    "forecast_df = pd.DataFrame(forecast, columns=columns,index=test_data.index)\n",
    "#Plot original data and forecasted values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df, label='Original Data')\n",
    "plt.plot(forecast_df, label='Forecast',linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.title('Regularized VAR Forecast')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Partical 4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "#Generate some example data\n",
    "np.random.seed(0)\n",
    "nobs = 100\n",
    "data = np.random.randn(nobs, 2)\n",
    "data[:, 1] = 2*data[:, 0] + np.random.randn(nobs) #Create a cointegrated relationship\n",
    "#Create a DataFrame from the generated data\n",
    "df = pd.DataFrame(data, columns=['series1', 'series2'])\n",
    "#Estimate the cointegration rank using the trace statistic\n",
    "coint_rank_result = select_coint_rank(df, det_order=-1, k_ar_diff=2)\n",
    "#Extract the cointegration rank from the result onject\n",
    "coint_rank = coint_rank_result.rank\n",
    "#Initialize and fit the VECM model\n",
    "model = VECM(df, k_ar_diff=2, coint_rank=coint_rank)\n",
    "results = model.fit()\n",
    "#Print the summary of the model\n",
    "print(results.summary())\n",
    "\n",
    "\n",
    "#Case study DDCM\n",
    "Topic: Factors Influencing Adoption of IoT for Data-driven Decision Making in Asset Management Organizations.\n",
    "Degree: Delft University of Technology, Netherlands\n",
    "Author: Paul Brous, Marijn Janssen, Daan Schraven, Jasper Spiegeler and Baris Can Duzgun\n",
    "Abstract :\n",
    "The Internet of Things (IoT) enables the creation of data that can be used to gain further insights into the current and \n",
    "predicted state of the infrastructure and may help automate the asset management process. The objective of this paper is to \n",
    "explore implementation factors for adoption of new data sources for decision-making in asset management organizations. Based \n",
    "on a systematic literature review and case studies in the asset management domain, this paper derives the current use and \n",
    "expectations of new data sources for decision-making in asset management. The paper concludes that although recent technological\n",
    "developments have enabled the deployment of IoT for asset management, the current level of adoption remains low. The inherent \n",
    "complexity of adopting a data driven approach to asset management requires an effective data governance strategy to ensure data\n",
    "quality, manage expectations, build trust and integrate IoT data in decision-making processes.\n",
    "Introduction :\n",
    "Many organizations tasked with managing civil infrastructure routinely store large volumes of data. More and more, new sources \n",
    "provide this data for producing and collecting real world data that can be communicated on the internet, such as sensor devices, social media, and user-generated data. The Internet of Things (IoT) describes a situation whereby physical objects are connected to the Internet and are able to communicate with, and identify themselves to, other devices. For example, this may include GPS-based navigation applications for smartphones based on real-time traffic information shared by other drivers, or real-time weather service based on the information updated by sensors of users’ smartphones or weather radars and other weather observation tools. This research takes place in the asset management domain of large scale civil infrastructure. Asset management (AM) is a discipline for optimizing and applying strategies related to work planning decisions in order to effectively and efficiently meet the desired objective.\n",
    "IoT is important to AM because an object that can communicate digitally also becomes connected to surrounding objects and \n",
    "data infrastructures. For example, it is possible to determine the position and length of traffic jams, and to monitor trends, \n",
    "variations, and relationships in the road network over time using smartphone data, networked sensors and cameras to analyse \n",
    "traffic flow. But in order for IoT data to be accepted by asset managers, a variety of barriers such as trust and acceptance \n",
    "still need to be overcome. The concept of trust is often used in various contexts and with different meanings. Trust is a \n",
    "complex notion which is hard to define, although its importance in data-driven decision-making is widely recognized.\n",
    "Case Studies :\n",
    "Two cases have been studied to identify how the adoption of IoT data is done by asset management organizations. In the first \n",
    "case we study the adoption of IoT data by a consortium for the maintenance of a bridge. In the second case we study the adoption\n",
    "of IoT data for the maintenance of the road sections of a highway between two cities in the east of the Netherlands.\n",
    "Case 1: Bridge Inspection with a Drone\n",
    "IoT is expected to enable remote sensing of the condition of bridges and enhance the available information on their condition \n",
    "if performed correctly. For this bridge, new methods of remote sensing have been tested, in the expectation to pilot with IoT \n",
    "sensors in the succeeding year to improve the quality of monitoring. For this case, the main driver for IoT and other forms of \n",
    "remote sensing appeared to be the lack of accessibility of some parts of the bridge for visual inspections. For example, \n",
    "locations above and below the bridge there is no space for setting up equipment (e.g. scaffoldings, boom lifters or ladders) \n",
    "such that visual inspector can work. This way, parts of the bridge remain poorly inspected, making it harder to physically \n",
    "detect local cases of bridge deterioration. In combination with the innovation program of RWS, the maintenance consortium used \n",
    "the pilot project to perform inspections with help of a drone that was equipped with a camera to observe the less reachable \n",
    "parts of the bridge, thereby increasing the operation’s efficiency. The drone inspection was also performed at better reachable \n",
    "parts to compare the inspection results of the drone against the inspection results of a human inspector. This comparison gave \n",
    "new data for the usefulness of adopting IoT data, since the use of drones during inspection was relatively new for bridge \n",
    "assessments. In terms of a strategic use of IoT data, the consortium judged that the obtained information was good enough to \n",
    "give a reliable overview of the found damages at the bridge parts, which were harder to reach. This shows that the decision \n",
    "support services and performance report could be based on a more complete view of bridge data. \n",
    "In terms of a tactical use of IoT data the consortium found that the bridge inspection with the drone resulted in less costs \n",
    "than a human inspection with the needed equipment to access the areas of the bridge. Therefore, the adoption of drones results \n",
    "in a reduction of costs with respect to inspecting a bridge. On the operational use of IoT data, the adoption of a drone showed \n",
    "practical constraints. The drone did not receive a GPS signal under the bridge deck which prohibited it to follow its \n",
    "predetermined flight route. Therefore, it had to be steered manually which made the process of documenting and keeping record of\n",
    "the locations of the taken inspection photographs more difficult and time consuming than expected. Secondly, the damages \n",
    "themselves were clearly visible but the extent and size were hard to measure from only the digital images. Thirdly, the \n",
    "drone had to fly at a minimum distance of 1.5 meters from the bridge components which resulted in the incapability of observing \n",
    "the bearings of the bridge and affected the completeness of the drone’s dataset. Finally, the bridge had to be closed off for \n",
    "traffic due to safety regulations. The consortium concluded that the use of drones is not ideal for assessing bridges. Another \n",
    "interesting side-note with regards to this case of adopting drones for bridge inspections, is that the consortium has made \n",
    "further plans to implement a pilot project using IoT sensors that communicate over a Long-Range Low-Power (LoRa) network to\n",
    "monitor bridge movements. Robust, smart wireless sensing systems that are suitable for use in civil engineering have been \n",
    "developed specially for this project, as well as the software to analyse and interpret the data. According to the interviewees, \n",
    "these new sensing methods should speed up and improve current monitoring methods\n",
    "\n",
    "\tIoT data expected to change performance measurement of infrastructure service\tIoT data expected to change perception of \n",
    "    infrastructure service\tIoT data expected to change improvement processes of infrastructure service\n",
    "Strategic use of IoT data\t• Decision support services (trend analysis) \n",
    "• Reporting \t• Communication of long term planning and strategic choices\n",
    " • Improve perceived optimization of services \t• Encourage proactive processes\n",
    "• Encourage self-organization\n",
    "• Determine strategic changes to infrastructure\n",
    "Tactical use of IoT data\t• Cost management \n",
    " • Time management \n",
    " • Planning \n",
    "• Post-events    evaluations \t• Communication of short term planning and actions \n",
    "• Improve perceived quality of services\n",
    "• Public enactment \t• Enable directed procedures \n",
    "• Enable efficient recovery \n",
    "• Control event occurrence \n",
    "• Improve utilization of existing infrastructure \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Case Study :Privacy and Legal\n",
    "Topic: The London Whale Case Study\n",
    "Faculty: Indian Institute of Management\n",
    "Author: Sumit Kumar,Pankaj Kumar Baag\n",
    "Abstract:\n",
    "The \"London Whale\" case study refers to a notable financial scandal involving JPMorgan Chase, one of the largest and most \n",
    "prominent financial institutions in the world. The case centers around large trading losses incurred by JPMorgan's \n",
    "Chief Investment Office (CIO) in 2012. The losses were initially attributed to a trader named Bruno Iksil, who worked \n",
    "at the bank's London office and became known as the \"London Whale\" due to the size of his trading positions.\n",
    "Introduction:\n",
    "The London Whale did cost JP Morgan Chase a whopping $6.2 billion loss in 2012 from trading in the synthetic credit portfolio. \n",
    "The case study raised important questions on the banks' addiction to risk and greed. In a way, what Chief Investment \n",
    "Officer (CIO) at JP Morgan did was the same old narrative. They doubled down after a loss from trade with even bigger \n",
    "gambles hoping that their big gambles would lead to a huge payday. Below is the timeline given in Table 1 culminating to \n",
    "the discovery of a financial scandal which the media christened the ‘London Whale’ However, plenty more was wrong. The CIO's \n",
    "job at the bank, it would appear, was to hold down the bank's level of credit risk. However, the CIO office used hundreds of \n",
    "billions of dollars at its disposal to speculate, as opposed to hedging, (a greater proportion emerged from deposits, which \n",
    "the company realized was always greater than the loans they issued) and to emerge as a money-maker. \n",
    "The CIO office was engaged in a web of complex derivative trading and little to do with hedging.\n",
    "The London Whale Case Study\n",
    "The London Whale in the case study had gone long and in a big way on credit default swaps, and this means that he sold \n",
    "vast amounts of protection against the credits making up the index. When a conference call with market analysts, Braunstein, \n",
    "who was the bank's CFO admitted that the positions meant to hedge investments the banks make in extremely top-grade assets \n",
    "with excess deposits. He also admitted that the chief investment position was made consistent with JP Morgan Chase's overall \n",
    "risk strategy. The dynamic hedging had assumed that the markets would remain efficient. This did not work out as thought. \n",
    "The huge trade had ended up distorting the index itself, i.e. the index became cheaper than the constituents. Hedge funds \n",
    "were aware of this big position and then used skew trades to \"express\" this view Hedge funds saw this difference and started a\n",
    "rbitrage. They waited for the divergence to die down. But the market did not correct. The London Whale was actually selling vast amounts or protections and then sitting on them to an extent that the market could not autocorrect. The trade unwinds factor. In the meantime, the person engaged in selling possibly developed some cold feet. As has been stated, the person possibly stopped re-balancing the hedged, and this possibly led to an increase in the risk as well as the losses. From the bank's 10-Q form, since March 2012, the chief investment office incurred substantially mark to market losses in its SCP and which has proven to be greatly risky, and even more volatile as well as not effective as a financial hedge that the firm had believed previously. In short, JP Morgan's trade ended up distorting the market but eventually led to a massive loss due to a large unhedged exposure. The loss only goes to show how weak the legal regulatory system in place. The CEO of JP Morgan Chase was adamant and possibly right, in admitting that Mr. Iksil's gamble was legally complaint, despite the fact that they obviously violated the spirit of the Volcker Rule, and possibly, the market regulators will listen more to the Occupier's letter because the bank has now possibly fashioned risk management.\n",
    "If JP Morgan Chase cannot do it, then there is no bank that can do it. He was hedging JP Morgan's credit risk, which is not \n",
    "marked to market. If corporate credit improved, you would expect that hedge to lose money, although JP Morgan as a firm would \n",
    "net profit, because the value of its loans to corporations and other businesses would go up. However, that's not why the whale \n",
    "lost money. The whale made complex bets. The bets were large enough to move the markets in their direction, which led to high \n",
    "paper profits and low reported risk. It's like if you buy a stock and keep buying, the price goes up and you look as if you're \n",
    "making money. Hedge funds noticed the prices going up, and took the other side, pushing prices back down and creating losses. \n",
    "These losses required JP Morgan to post cash collateral. Also, the losses made the positions look much riskier to the bank's \n",
    "risk models. Senior management decided to back away from the strategy, which locked in the losses but prevented further losses. \n",
    "The basic story is a common one. The bank had more risk exposure than it wanted, in this case to European corporate credit risk. This is inherent to a bank's business; part of its job is to understand credit risk and make bets on it. But it also looks for cost-effective ways to lay off some of that risk, just like an insurance company might buy reinsurance against an unexpectedly large amount and size of claims. JP Morgan appointed a trader to lay off European credit risk in an opportunistic manner. That is, they didn't want him just to buy general protection at the going rate, but to pick and choose, buying protection when and where it was cheap, and retaining credit risk when and where protection was overpriced. The trader went further and actually sold protection when and where it was overpriced.\n",
    "Of course, it's also possible that the decision would have been to cancel the strategy; or to leave the strategy under the \n",
    "existing risk managers. But even in the latter case, the CEO would not have been surprised and embarrassed when the losses \n",
    "came to light. Arguably, that did as much damage as the losses themselves. Instead of denying the losses, he would have said \n",
    "they were part of a hedging strategy under risk control, that management and the board had confidence in, and that the mark \n",
    "to market losses was acceptable in light of the long-term program goals. Instead of doing a fire-sale at the worst time, the \n",
    "program could have continued until it returned to profitability, or been closed down deliberately to minimize losses. So how \n",
    "did the risk management message get lost in translation? We think the line-level risk management was unimaginative and timid, \n",
    "but not incompetent. This is the only way to establish clear responsibility for risk. It's not a question of pointing fingers \n",
    "afterward; it's making sure authority and responsibility are in the same place. As it happens, we think authority was pushed \n",
    "down too low, and responsibility pushed up too high, so when losses mounted the people in a position to do something about it \n",
    "didn't feel responsible, and the people responsible didn't have the ability to fix it.\n",
    "\n",
    "Conclusion :\n",
    "First, there are matters of law and regulation. For example, there are regulations and some laws about accurately \n",
    "describing your business strategy and reporting/calculating risk measures if you are a public company. JPM arguably \n",
    "failed to meet such standards of corporate governance at multiple levels. Among the self-identified failures and weaknesses \n",
    "were misleadingly “favourable” valuations of various positions at different times, failure to execute and monitor a stated \n",
    "strategy, identifying a hedge that had no identifiable hedging characteristics or justification, failure to enforce \n",
    "self-identified risk limits, failure to disclose material information to regulators and investors, and other things. \n",
    "Some argue that this does not prove the need for government involvement, or alternatively that the best and only role for \n",
    "government is to enforce contracts or ensure accurate information so that market discipline will do the rest. For the case of \n",
    "JP Morgan, statements from the internal and external structures varied, leading to instances of misconduct due to the lack \n",
    "of ethical considerations by the CIO and CFO, whose actions or inactions directly led to the bank making significant losses. \n",
    "Therefore, use of the virtue ethics and instrumental ethics in any financial setting tries to settle the likelihood of future \n",
    "and current financial discrepancies due to poor controls among the senior and subordinate management. On the other hand, \n",
    "ethics might be accompanied by human imperfections since some individuals are hard to implement virtue ethics and put in \n",
    "practice. This affects the wellbeing of individuals and on how they handle issues at the corporate level. The importance of \n",
    "these were demonstrated fairly conclusively during the great depression in the US and the recent financial crisis, as well as \n",
    "a long history of bank failures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Case Study:Legal Ascepts\n",
    "Topic : A case study on Analysis under GDPR\n",
    "Degree : Faculty of Computer Science and Electrical Engineering, Kiel University of Applied Science\n",
    "Author : Nils Gruschka, Vasileios Mavroeidis, Kamer Vishi , Meiko Jensen\n",
    "Abstract :\n",
    "Big data has become a great asset for many organizations, promising improved operations and new business opportunities. However, big data has increased access to sensitive information that when processed can directly jeopardize the privacy of individuals and violate data protection laws. As a consequence, data controllers and data processors may be imposed tough penalties for non-compliance that can result even to bankruptcy. In this paper, we will discuss case studies dealing with sensitive data and actions for complying with the data regulation laws. We show which types of information might become a privacy risk, the employed privacy preserving techniques in accordance with the legal requirements, and the influence of these techniques on the data processing phase and the research results.\n",
    "Introduction :\n",
    "The term big data describes large or complex volumes of data, both structured and unstructured that can be analysed to bring \n",
    "value. The typical definitions refer to big data by a number of V-properties, such as volume, velocity, and variety. Today, big\n",
    "data has become capital, with enterprises improving substantially their operations and customer relations, and the academia \n",
    "developing and enhancing research. In addition, the huge amount, generation speed, and diversity of data require special \n",
    "architectures for storage and processing (e.g., MapReduce or Apache Hive). In order to protect individuals and their data \n",
    "a number of technical means and regulations for privacy-preserving data processing have been initiated and developed. However, \n",
    "implementing these methods in a data processing system obviously requires additional effort during the design phase, and in \n",
    "many cases such methods influence the performance of the system. As a result, in the past, enterprises and other organizations \n",
    "were not always willing to make this effort, but this tend to change due to the pressure applied from new privacy laws and \n",
    "regulations. This paper describes privacy issues in big data analysis and elaborates on two case studies in order to elucidate \n",
    "how legal privacy requirements can be met in research projects working on big data and highly sensitive personal information. \n",
    "Finally, it discusses resulted impacts on the processing of data and the results due to the employed privacy-preserving \n",
    "techniques.\n",
    "Privacy Issues in Big Data Analysis\n",
    "A. Legal Regulations\n",
    "From a legal point of view, in this paper we focus on the EU General Data Protection Regulation (GDPR), which came into force \n",
    "in May 2018. It is relevant to all organizations inside the European Union (EU), the European Economic Area (EEA) and also to \n",
    "organizations from other countries, if they process data of European citizens. Thus, the GDPR has effect on most major \n",
    "companies worldwide. The GDPR regulates the collection, storage, and processing of personal data. Personal data are any \n",
    "data that can be linked to a specific natural person. This includes not only direct personal identifiers (e.g., full name, \n",
    "national ID number) but also indirect identifiers like phone numbers, IP addresses, or photos with identifiable people. Data \n",
    "that do not include such identifiers are commonly regarded as anonymous and are outside the scope of GDPR. The results of big \n",
    "data analysis are very often statistical findings without direct links to specific individuals. Hence, a simple method to \n",
    "conform to all requirements of GDPR is to process only anonymous data.\n",
    "A famous example of re-identification is the Netflix challenge in 2006. As part of a competition for finding more accurate \n",
    "movie recommendation methods, Netflix released a dataset containing movie ratings of 500,000 customers. In the dataset, any \n",
    "personally identifiable information (PII) was removed and only subscriber IDs (without any connection to the actual identity) \n",
    "and movie ratings (score, movie info, date) were published. However, researchers combined these data with other publicly \n",
    "available information (e.g., IMDB ratings) and were able to identify individual customers with a high probability. Other \n",
    "well-known cases include identification of individuals from internet search terms, anonymized DNA and mobility data.\n",
    "Case Study : Oslo Analysis\n",
    "The Operable Subjective Logic Analysis Technology for Intelligence in Cybersecurity is a research project funded under \n",
    "the ICT and Digital Innovation program of the Research Council of Norway for the University of Oslo for the period of \n",
    "2016 – 2019. Oslo Analytics develops advanced analytical methods based on big data analysis, machine learning and subjective \n",
    "logic to gain a deep situational awareness and understanding of security incidents. The project is organized in collaboration \n",
    "with national and international institutions, organizations and security vendors such as the Norwegian Computing Center (NR), \n",
    "the Norwegian National Security Authority (NSM), The Defence Intelligence College.The Norwegian Centre for Research Data (NSD) \n",
    "is responsible for implementing the statutory data privacy requirements in the research community, and thus requires \n",
    "notification from every research project processing personal data that are not fully anonymized. Fully anonymous data are \n",
    "information that cannot in any way identify an individual either directly through name and national identity number, or \n",
    "indirectly through background variables, a name list, scrambling key, encryption formula or code.\n",
    "1. Handling Sysmon Data – End Point Security\n",
    "Data of particular research importance for Oslo Analytics are Sysmon logs. Sysmon is a Windows system service and device \n",
    "driver that monitors and logs system activity of Windows workstations, servers and domain controllers. Sysmon provides some \n",
    "of the most effective events needed to trace attacker activity and increase host visibility. Sysmon event class \n",
    "”Network Connection” with ID 3 can be used to identify network activity, such as connections to command and control servers \n",
    "(C&C) or even download encryption keys.\n",
    "Like many other datasets, Sysmon contains multiple privacy-sensitive identifiers (Windows account usernames, computer \n",
    "names, static internal IPs) and user-behaviour (running processes, internet activity) that Oslo Analytics has to deal with \n",
    "prior processing.\n",
    "2. Data Storage and Accessibility\n",
    "The data are stored on a secure server with access restricted to authorized researchers working on Oslo Analytics \n",
    "under a very tight access control list adopting the principle of least privilege. Processing of the data can only \n",
    "occur on the server. Access to the secure server is only allowed from inside the organizational network and this \n",
    "is restricted to specific computers filtered by their MAC addresses, their internal static IP, and user account. \n",
    "In addition, a firewall has been configured to allow only incoming connections to the server on port 22 (SSH). \n",
    "Any other network activity is denied and consequently dropped. In this respect, the network restrictions \n",
    "disallowed us to personally install any extra programming libraries needed for processing the data after \n",
    "setting up the server. Thus, we had to inform the security team that is responsible for the security of the server \n",
    "and the data stored. Finally, the user accounts for processing the data on the secure server are only valid for the \n",
    "duration of the project (account expiration), meaning that the accounts will be disabled on a specific date. The same \n",
    "principle applies to the Sysmon data which restricts the duration of the data storage to the active period of the project.\n",
    "Conclusion :\n",
    "This paper presented the implications of data protection laws on projects dealing with big data, and by using case \n",
    "studies analysed how privacy-preserving techniques can be applied. The results were quite different. Mitigating \n",
    "privacy concerns regarding biometric data collection and processing the participants were asked to give consent. \n",
    "In addition, no problems were faced during the data analysis phase. Data from an existing data source were used. \n",
    "Here, anonymization of many data fields was required, making the data analysis more challenging and in many cases limited. \n",
    "It is of great importance to remark that for projects and technologies dealing with sensitive data a data protection impact \n",
    "assessment should be conducted at the very early stages of the project to identify potential privacy challenges, and to \n",
    "adapt the analysis methods taking into consideration privacy-preserving techniques.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Case Study: Emerging Technology\n",
    "Topic : Big Data Emerging Technologies : A Case Study with Analyzing Twitter Data Using Apache Hive\n",
    "Faculty : Computer Science & Engineering Department\n",
    "Author : National Institute of Technical Teachers Training and Research, Chandigarh, India \n",
    "Abstract :\n",
    "These are the days of Growth and Innovation for a better future. Now-a-days companies are bound to realize need of Big Data to make decision over complex problem. Big Data is a term that refers to collection of large datasets containing massive amount of data whose size is in the range of Petabytes, Zettabytes, or with high rate of growth, and complexity that make them difficult to process and analyze using conventional database technologies. Big Data is generated from various sources such as social networking sites like Facebook, Twitter etc, and the data that is generated can be in various formats like structured, semi-structured or unstructured format. For extracting valuable information from this huge amount of Data, new tools and techniques is a need of time for the organizations to derive business benefits and to gain competitive advantage over the market. In this paper a comprehensive study of major Big Data emerging technologies by highlighting their important features and how they work, with a comparative study between them is presented. This paper also represents performance analysis of Apache Hive query for executing Twitter tweets in order to calculate Map Reduce CPU time spent and total time taken to finish the job.\n",
    "Introduction :\n",
    "Digital universe is flooded with large amount of data generated by number of users worldwide. These data are of diverse in \n",
    "nature, come from various sources and in many forms. Every time we use Internet, send an email, make a phone call, or pay a \n",
    "bill, we create data. All this data needs to be stored in huge data chunks. These data chunks are stored in thousands of disks \n",
    "or hard drives. Around 2.72 zettabytes of data were created until 2012 and it is expected to double every two years reaching \n",
    "about 8 zettabytes at the end of 2015. Multimedia industries and increase use of social networking sites are the major source \n",
    "of Big Data generation. Every minute Facebook users shares nearly 3.3 million pieces of content, Twitter user sent 347,22 \n",
    "tweets, 100 hours of videos uploaded on YouTube, and 4.1 million search queries are executed on Google every minute. So, \n",
    "in order to get business value from this large amount of data generated Hadoop and its Ecosystems are the popular solution \n",
    "which can help out for better Big Data Analytics solution. This paper is organized as follows. An overview of BigData and \n",
    "Hadoop is presented. Hadoop Ecosystem with analyzing Twitter data by using Apache Hive configured on Microsoft HDInsight \n",
    "Hadoop cluster is discussed. \n",
    "Hadoop For Big Data Processing\n",
    "The main challenge in front of IT world is to store and analyze huge quantities of data. Every single day data is generated \n",
    "in huge amount from various fields like Geography, Engineering, and Economics & Science etc. To analyze such huge amounts \n",
    "of data for better understanding of users there is a need to develop data intensive applications which are highly available, \n",
    "highly scalable and based on the reliable storage system. To cope up with these requirements in 2003, Google developed \n",
    "Distributed File System (DFS) called Google File System GFS and introduced MapReduce programming model to achieve high \n",
    "performance by moving tasks to the nodes where the data is stored and by executing them in parallel. GFS was a great \n",
    "discovery in order to handle massive data for storing, retrieving, processing and analyzing. But, the major issue with \n",
    "GFS was that this file system was proprietary, so the researcher team of Yahoo developed an open source implementation of \n",
    "GFS and Map-Reduce and later this open-source project was named as Apache Hadoop. Hadoop was created by Doug Cutting, an \n",
    "employee at Yahoo for the Nutch search engine project. By seeing his son’s toy elephant Doug named it as Hadoop with yellow \n",
    "elephant like symbol. Hadoop architecture mainly comprise of two main components: HDFS for storing Big Data and MapReduce \n",
    "for Big Data analytics.\n",
    "HDFS Architecture :\n",
    "Hadoop Distributed File System(HDFS) is a file system which is used for storing large datasets in a default block of size \n",
    "64 MB in distributed manner on Hadoop cluster. Hadoop cluster means running a set of daemons on different servers of the \n",
    "network.\n",
    "MapReduce Architecture :\n",
    "a. Map Stage\n",
    "Map is a function that splits up the input text, so map function is written in such a way that multiple map jobs can be \n",
    "executed at once, map is the part of the program that divide up the tasks. This function takes key/value pairs as input \n",
    "and generates an intermediate set of key/value pairs.\n",
    "b. Reduce Stage\n",
    "Reduce is a function that receives the mapped work and produces the final result. The working of Reduce function \n",
    "depends upon merging of all intermediate values associated with the same intermediate key for producing the final result.\n",
    "Case Study for Analysing Twitter Data\n",
    "Experimental Setup\n",
    "Social websites like Twitter are a useful source of Big-Data for analyzing and understanding users trends. \n",
    "The main objective of this work is to fetch and analyze Twitter tweets which are stored as JavaScript Object Notation \n",
    "(JSON) format on cloud based Apache Hive solution. For the implementing of this work we have used Microsoft Azure cloud \n",
    "services. Two Infrastructure- as a Service (IaaS) services were used: one is HDInsight Hadoop solution. First, Twitter \n",
    "live data were fetched by using Twitter Streaming API and then fetched raw data was stored into Blob Storage after \n",
    "that it is transferred into Hive Table. Now HDInsight cluster of various nodes size was created on which Apache \n",
    "Hive queries were executed to analyze the Twitter tweets.\n",
    "Hive query was executed on the data stored in Hive table and the results of a number of tweets count were calculated. \n",
    "The result of HDInsight cluster for running Hive query were analyzed based on two parameters: first one is Total Map \n",
    "    Reduce CPU Time Spent for running Hive query and second is Total Time taken for running this job. HDInsight Hadoop \n",
    "    cluster of size 1 node, 2 nodes, 4 nodes, and 6 nodes are used. The results for mapreduce CPU time spent on Hadoop \n",
    "    cluster measured in seconds when the hive query was executed for feteching and predicting tweets count. \n",
    "    It is observed that as the number of nodes in HDInsight cluster increase the mapreduce slot time for executing \n",
    "    hive query increase because more number of nodes in cluster means more switching of mapper and reducer function \n",
    "    on the cluster nodes. It is observed that as the number of nodes in HDInsight cluster increase total time taken \n",
    "    to execute Hive query decrease because if we increase number of nodes in HDInsight cluster then processing of \n",
    "    Hive query can take place parallely and which will decrease the query execution time.\n",
    "Conclusion :\n",
    "Big Data analysis is the latest area of interest for the research communities around the globe. Big Data refers to \n",
    "the volume of data beyond the traditional database technology capacity to store, access, manage and compute efficiently. \n",
    "By analyzing this large amount of data companies can predict the customer behavior, improved marketing strategy, and get \n",
    "competitive advantages in the market. Hadoop is a flexible and open source implementation for analyzing large datasets \n",
    "using MapReduce. There are various emerging technologies such as Apache Pig, Hive, Sqoop, HBase, Zookeeper, and Flume \n",
    "that can be used to improve the performance of basic Hadoop MapReduce framework. Apache Pig which is a scripting language \n",
    "that can be used to reduce development time of MapReduce program because it requires less number of lines of code and \n",
    "provides nested data types that are missing from MapReduce. Hive provides easy to use platform for the developers who \n",
    "are comfortable in SQL language for Map Reduce programming. HDFS has the inability of random read/write to BigData that \n",
    "can be provided by HBase. If we want to transfer data between Hadoop and RDBS system Sqoop can be used. Zookeeper can be \n",
    "used for synchronization of Hadoop cluster and finally Flume can be used for moving streaming web log data to HDFS. \n",
    "This paper also discussed fetching and executing Twitter tweets by using Hive query on HDInsight cluster and results \n",
    "shows that as we increase number of nodes in the cluster.\n",
    "Case Study for Analysing Twitter Data\n",
    "Experimental Setup\n",
    "Social websites like Twitter are a useful source of Big-Data for analyzing and understanding users trends. \n",
    "The main objective of this work is to fetch and analyze Twitter tweets which are stored as JavaScript Object Notation (JSON) \n",
    "format on cloud based Apache Hive solution. For the implementing of this work we have used Microsoft Azure cloud services. \n",
    "Two Infrastructure- as a Service (IaaS) services were used: one is HDInsight Hadoop solution. First, Twitter live data were \n",
    "    fetched by using Twitter Streaming API and then fetched raw data was stored into Blob Storage after that it is \n",
    "    transferred into Hive Table. Now HDInsight cluster of various nodes size was created on which Apache Hive queries were \n",
    "executed to analyze the Twitter tweets.\n",
    "Hive query was executed on the data stored in Hive table and the results of a number of tweets count were calculated. The result of HDInsight cluster for running Hive query were analyzed based on two parameters: first one is Total Map Reduce CPU Time Spent for running Hive query and second is Total Time taken for running this job. HDInsight Hadoop cluster of size 1 node, 2 nodes, 4 nodes, and 6 nodes are used. The results for mapreduce CPU time spent on Hadoop cluster measured in seconds when the hive query was executed for feteching and predicting tweets count. It is observed that as the number of nodes in HDInsight cluster increase the mapreduce slot time for executing hive query increase because more number of nodes in cluster means more switching of mapper and reducer function on the cluster nodes. It is observed that as the number of nodes in HDInsight cluster increase total time taken to execute Hive query decrease because if we increase number of nodes in HDInsight cluster then processing of Hive query can take place parallely and which will decrease the query execution time.\n",
    "Conclusion :\n",
    "Big Data analysis is the latest area of interest for the research communities around the globe. \n",
    "Big Data refers to the volume of data beyond the traditional database technology capacity to store, access, \n",
    "manage and compute efficiently. By analyzing this large amount of data companies can predict the customer behavior, \n",
    "improved marketing strategy, and get competitive advantages in the market. Hadoop is a flexible and open source \n",
    "implementation for analyzing large datasets using MapReduce. There are various emerging technologies such as \n",
    "Apache Pig, Hive, Sqoop, HBase, Zookeeper, and Flume that can be used to improve the performance of basic Hadoop \n",
    "MapReduce framework. Apache Pig which is a scripting language that can be used to reduce development time of MapReduce \n",
    "program because it requires less number of lines of code and provides nested data types that are missing from MapReduce. \n",
    "Hive provides easy to use platform for the developers who are comfortable in SQL language for Map Reduce programming. \n",
    "HDFS has the inability of random read/write to BigData that can be provided by HBase. If we want to transfer data between \n",
    "Hadoop and RDBS system Sqoop can be used. Zookeeper can be used for synchronization of Hadoop cluster and finally Flume \n",
    "can be used for moving streaming web log data to HDFS. This paper also discussed fetching and executing Twitter tweets by \n",
    "using Hive query on HDInsight cluster and results shows that as we increase number of nodes in the cluster.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
